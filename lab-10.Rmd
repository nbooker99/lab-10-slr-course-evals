---
title: "Lab 10 - Grading the professor, Pt. 1"
author: "Noah Booker"
date: "4/5/25"
output: github_document
---

## Load Packages and Data

```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse) 
library(tidymodels)
library(openintro)
```

# Part 1: Exploratory Data Analysis

## Exercise 1

> Visualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response.

```{r score_dist}
evals %>% 
  ggplot(aes(x = score)) +
  geom_histogram() +
  theme_minimal()
```

The histogram above reveals that the there is a negative skew in the distribution of avaergae professor evaluation scores.

This reflects the fact that most student evaluations are towards the more postive end of the scale, and few are negative.

This aligns with what I might've guessed. I would guess that it's pretty rare that teachers get very low evaluations.

## Exercise 2

> Create a scatterplot of score versus bty_avg (a professor’s average beauty rating). Describe any pattern you observe—does there appear to be a trend, clustering, or wide variation? Don’t overthink it; just describe what you see.


```{r score_x_beauty}
evals %>% 
  ggplot(aes(x = bty_avg, y = score)) +
  geom_point() +
  theme_minimal()
```

Scores have possible values of 1-5 and bty_avg has possible values of 1-10. There is a fairly high density of high scores (between 4 and 5) across the range of avearge beauty scores. However, in the lower range of beauty ratings (2-5), there is a fairly high density of medium evaluation scores (between 3 and 4) and some scores below  3, while in the higher range of beauty ratings (6 to 8 and above), there are only evaluatoin scores above 3, and there seem to be a greater density of scores above 4.

## Exercise 3

> Recreate your scatterplot from Exercise 2, but use geom_jitter() instead of geom_point(). What does jittering do, and why might it improve the plot? Was anything misleading or hidden in the original version?

```{r jitter}
evals %>% 
  ggplot(aes(x = bty_avg, y = score)) +
  geom_jitter() +
  theme_minimal()
```

Jittering reveals the points that were laying on top of each other in the original plot, showing the spots of the plot where there are a greater density of points than we would have seen otherwise. My interpretation of the patterns of the plot, however, is basically the same.

# Part 2: Linear regression with a numerical predictor

## Exercise 4

> Let’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model.

```{r m_bty}
m_bty <- summary(lm(score ~ bty_avg, data = evals))
m_bty
```

```{r m_bty_tidymodels}
linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals) %>%
  tidy()
```

Based on the results of the regression analysis the model predicting average professor evulation scores from average beauty rating is:

score = 3.89 + .07 x bty_avg + error

## Exercise 5

> Replot your visualization from Exercise 3, this time add a regression line in orange. Turn off the default shading around the line. By default, the plot includes shading around the line—what does that shading represent? And speculate why I’m asking you to turn it off.

```{r plot_w_regression_line}
evals %>% 
  ggplot(aes(x = bty_avg, y = score)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  theme_minimal()
```

The default shading around the line represents the 95% confidence interval, I think. I don't know why you're asking us to turn it off—it seems like good information to have in the plot. Perhaps, it's so that we know how to turn it off, should we want to in the future.

## Exercise 6

> What does the slope of the model tell you? Interpret it in the context of this data—what does it say about how evaluation scores change with beauty ratings?

The slope of .07 indicates that, in the model, every 1-unit increase in average beauty rating is associated with a .07 increase in average professor evaluation score.

## Exercise 7

> What does the intercept represent in this model? Is it meaningful in this context, or just a mathematical artifact? Explain your reasoning.

The intercept in this model, 3.89,  represents the predicted average professor evaluation score for a professor with an average beauty rating of 0. It's meaningfulness in this context is limited by the fact that there are no professors with an average beauty rating of 0 and by the fact that we may not be interested in the predicted average professor evaluation score for a professor with an average beauty rating of 0. If we mean-centered the average beaty scores, then the intercept would represent the professor evaluation score for a professor with the mean beauty rating, which would be more interesting to know.

## Exercise 8

> What is the R-squared value of this model? Interpret it in context: how much of the variation in evaluation scores is explained by beauty ratings?

The R-squared in this model indicates that beauty ratings explain 3.50% of variation in evaluation scores.

# Part 3: Linear regression with a categorical predictor

> Let’s switch gears from numeric predictors to categorical ones. Beauty scores might be (somewhat) continuous, but characteristics like gender and rank are categorical, meaning they fall into distinct groups.

> We’ll start by seeing whether evaluation scores differ by gender.

```{r gender_score}
m_gen <- lm(score ~ gender, data = evals)
tidy(m_gen)
summary(m_gen)
```

## Exercise 9

> Take a look at the model output. What’s the reference level? What do the coefficients tell you about how evaluation scores differ between male and female professors?

R automatically created a dummy variable called gendermale in which female is the reference category, and the coefficient indicates the association of being male (versus female) on evaluation score. The coefficient indicates that, in the model, male professors have evaluation scores .14 points higher than female professors on average.

## Exercise 10

> What is the equation of the line corresponding to male professors? What is it for female professors?

male professors:    score = 4.09 + .14 x gendermale + error
female professors:  score = 4.23 + -.14 x genderfemale + error

## Exercise 11

> Fit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.

```{r rank_score}
m_rank <- lm(score ~ rank, data = evals)
summary(m_rank)
contrasts(evals$rank)
```

score = 4.28 + -0.13 x rank_tenure_track + -0.15 x rank_tenured

Because teaching professors are the reference category, the intercept of the model, 4.28, represents the average evaluations score for teaching professors. The slopes indicate that tenure track professors, on average, get evaluation scores .13 points lower than teaching professors (marginally significant) and that tenured professors, on average, get evaluation scores .15 points lower than teaching professors.

## Exercise 12

> Create a new variable called rank_relevel where "tenure track" is the baseline level. Hint: The `relevel()` function can be helpful!

```{r rank_relevel}
evals <- evals %>%
  mutate(rank_relevel = relevel(rank, ref = "tenure track")) #Claude helped.
contrasts(evals$rank_relevel)
```

## Exercise 13

> Fit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in the previous exercise. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the R-squared of the model.

```{r m_rank_relevel}
m_rank_relevel <- summary(lm(score ~ rank_relevel, data = evals))
m_rank_relevel
```

score = 4.15 + .13 x rank_teaching + -.02 x rank_tenured + error

The intercept of this model, 4.15, indicates the average evaluation score for tenure track professors. The slopes indicate that teaching professors, on average, have an evaluation score .13 points higher than tenure track professors (marginally significant) and that tenured professors, on average, have an evaluation score .02 points lower than tenure track professors (but this is not statistically significant, so we would say that they is no significant difference).

The R-squared indicates that differences in professors' rank explains only 1.16% of variance in evaluation scores.

## Exercise 14

> Create another new variable called tenure_eligible that labels "teaching" faculty as "no" and labels "tenure track" and "tenured" faculty as "yes".

```{r tenure_eligible}
evals <- evals %>% 
  mutate(tenure_eligible = case_when(
    rank == "teaching" ~ "no",
    rank %in% c("tenure track", "tenured") ~ "yes"
  ))
```

## Exercise 15

> Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the R-squared of the model.

```{r m_tenure_eligible}
m_tenure_eligible <- summary(lm(score ~ tenure_eligible, data = evals))
m_tenure_eligible
```

score = 4.28 + -0.14 x tenure_eligible_yes + error

The slope of this model, 4.28, represents the average evaluation score for professors who are not eligible for tenure (teaching professors). The slope indicates that professors who are eligible for tenure (tenure track or tenured professors) have, on average, an evaluation score .14 points lower than teaching professors (this difference is statistically significant).

The R-squared indicates that whether a professor is tenure eligible or not explains 1.15% of variance in evalutation scores.